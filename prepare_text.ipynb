{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b013356-6381-40e7-8976-24318599af7c",
   "metadata": {},
   "source": [
    "# LLM Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b8b8a0-6c28-4120-9087-dc3521284b0b",
   "metadata": {},
   "source": [
    "## Preparing arbitrary text for LLM training data\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    InputText --> TokenizedText --> TokenIDs --> TokenEmbeddings --> Transformer\n",
    "```\n",
    "\n",
    "1. Input Text is the raw sequence of arbitrary characters.\n",
    "2. *Tokenizer* parses the arbitrary stream of input characters to tokens\n",
    "3. Map each unique Token to an ID\n",
    "4. Convert those Token IDs into token embeddings, suitable for input to an LLM for pretraining\n",
    "\n",
    "\n",
    "```mermaid\n",
    "\n",
    "flowchart LR\n",
    "    inputtext[\"My name is Rob.\"] --> tokenizedtext[\"['My', 'name', 'is', 'Rob', '.']\"] --> tokenids[\"[40134, 2052, 133, 389, 12]\"] --> tokenembeddings[\"[[0.12, -0.45, ...], [0.34, 0.88, ...], ...]\"] --> transformer[\"Encoder/Decoder/Seq-to-Seq\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0a5fea-6057-4692-b4a8-1afc9971930e",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "---\n",
    "title: \"1 to 1 mapping of: token -> ID -> embedding vector\"\n",
    "---\n",
    "\n",
    "flowchart LR\n",
    "    subgraph inputtexts[\"Tokens\"]\n",
    "        direction TB\n",
    "        txt1[\"My\"]\n",
    "        txt2[\"name\"]\n",
    "        txt3[\"is\"]\n",
    "        txt4[\"Rob\"]\n",
    "        txt5[\".\"]\n",
    "    end\n",
    "    subgraph tokenids[\"Token IDs\"]\n",
    "        direction TB\n",
    "        id1[\"40134\"]\n",
    "        id2[\"2052\"]\n",
    "        id3[\"133\"]\n",
    "        id4[\"389\"]\n",
    "        id5[\"12\"]\n",
    "    end\n",
    "\n",
    "    subgraph embeddings[\"Token Embeddings (Seq × Dim)\"]\n",
    "        direction TB\n",
    "        e1[\"e₁ = [0.12, -0.45, 0.67, ...]\"]\n",
    "        e2[\"e₂ = [0.34, 0.88, -0.12, ...]\"]\n",
    "        e3[\"e₃ = [0.05, 0.22, 0.91, ...]\"]\n",
    "        e4[\"e₄ = [0.77, -0.56, 0.11, ...]\"]\n",
    "        e5[\"e₅ = [0.03, 0.44, -0.78, ...]\"]\n",
    "    end\n",
    "\n",
    "    txt1 --> id1\n",
    "    txt2 --> id2\n",
    "    txt3 --> id3\n",
    "    txt4 --> id4\n",
    "    txt5 --> id5\n",
    "    id1 --> e1\n",
    "    id2 --> e2\n",
    "    id3 --> e3\n",
    "    id4 --> e4\n",
    "    id5 --> e5\n",
    "\n",
    "    input[\"...\"] --> inputtexts\n",
    "    embeddings --> transformer[\"...\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7b26b7a-6800-4232-ba72-89b21367f72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 12, 4)\n",
      "2.5.1+cu121\n",
      "torch.cuda.is_available() = True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "print((sys.version_info.major, sys.version_info.minor, sys.version_info.micro))\n",
    "print(torch.__version__)\n",
    "print(f'torch.cuda.is_available() = {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4da532c-6d12-49b0-84dc-148a558c9815",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_ids = torch.tensor([2, 3, 5, 1])\n",
    "\n",
    "vocab_size = 6               # len(vocabulary)\n",
    "output_dim = 3               # len(embedding_vector), GPT-3 uses 12,288 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24914445-7019-4a2f-9a94-8566717808f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(6, 3)\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)       # to replicate on subsequent runs\n",
    "\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer)\n",
    "print()\n",
    "print(embedding_layer.weight)               # embedding layer's underlying weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96c2e3d7-d2dc-404f-bafb-2ed3f6c9f660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer(torch.tensor([3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7175f8cd-3773-4721-a3b3-a1cb86958ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 1.2753, -0.2010, -0.1606], grad_fn=<EmbeddingBackward0>),\n",
       " tensor([-0.4015,  0.9666, -1.1481], grad_fn=<EmbeddingBackward0>),\n",
       " tensor([-2.8400, -0.7849, -1.4096], grad_fn=<EmbeddingBackward0>),\n",
       " tensor([0.9178, 1.5810, 1.3010], grad_fn=<EmbeddingBackward0>)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[embedding_layer(x) for x in input_token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "012c96b0-f0b5-4a35-89a4-7c58214d389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max_length+1\"\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4f9dd61-1a28-4ff7-be2c-53a186a1bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d3ee561-df7d-49a1-bced-b9004219d602",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/pretraining-text.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "context_length = 1024\n",
    "\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "\n",
    "batch_size = 8\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text,\n",
    "    batch_size=batch_size,\n",
    "    max_length=max_length,\n",
    "    stride=max_length\n",
    ")\n",
    "\n",
    "it = iter(dataloader)\n",
    "inputs, targets = next(it)\n",
    "\n",
    "token_embeddings = token_embedding_layer(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "522ea1a4-dbe3-4e8b-8e4d-dcf522f0551b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n",
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "\n",
    "print(pos_embeddings.shape)\n",
    "\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cf6e35-06aa-4a34-b0cb-3b22a1dfb871",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "---\n",
    "title: \"1 to 1 mapping of: token -> ID -> embeddings (token_embed + positional_embed = input_embed)\"\n",
    "---\n",
    "\n",
    "flowchart LR\n",
    "    subgraph inputtexts[\"Tokens\"]\n",
    "        direction TB\n",
    "        txt1[\"My\"]\n",
    "        txt2[\"name\"]\n",
    "        txt3[\"is\"]\n",
    "        txt4[\"Rob\"]\n",
    "        txt5[\".\"]\n",
    "    end\n",
    "\n",
    "    subgraph tokenids[\"Token IDs\"]\n",
    "        direction TB\n",
    "        id1[\"40134\"]\n",
    "        id2[\"2052\"]\n",
    "        id3[\"133\"]\n",
    "        id4[\"389\"]\n",
    "        id5[\"12\"]\n",
    "    end\n",
    "\n",
    "    subgraph tokenembeddings[\"Token Embeddings (Seq × Dim)\"]\n",
    "        direction TB\n",
    "        e1[\"t₁ = [0.12, -0.45, 0.67, ...]\"]\n",
    "        e2[\"t₂ = [0.34, 0.88, -0.12, ...]\"]\n",
    "        e3[\"t₃ = [0.05, 0.22, 0.91, ...]\"]\n",
    "        e4[\"t₄ = [0.77, -0.56, 0.11, ...]\"]\n",
    "        e5[\"t₅ = [0.03, 0.44, -0.78, ...]\"]\n",
    "    end\n",
    "\n",
    "    subgraph posembeddings[\"Positional Embeddings (Seq × Dim)\"]\n",
    "        direction TB\n",
    "        p1[\"p₁ = [0.01, 0.03, -0.02, ...]\"]\n",
    "        p2[\"p₂ = [0.05, -0.11, 0.08, ...]\"]\n",
    "        p3[\"p₃ = [-0.07, 0.04, 0.09, ...]\"]\n",
    "        p4[\"p₄ = [0.10, -0.02, 0.05, ...]\"]\n",
    "        p5[\"p₅ = [0.00, 0.12, -0.06, ...]\"]\n",
    "    end\n",
    "\n",
    "    subgraph inputembeddings[\"Input Embeddings (Token ⊕ Positional)\"]\n",
    "        direction TB\n",
    "        i1[\"t₁ + p₁\"]\n",
    "        i2[\"t₂ + p₂\"]\n",
    "        i3[\"t₃ + p₃\"]\n",
    "        i4[\"t₄ + p₄\"]\n",
    "        i5[\"t₅ + p₅\"]\n",
    "    end\n",
    "\n",
    "    %% Addition nodes\n",
    "    add1((\"⊕\"))\n",
    "    add2((\"⊕\"))\n",
    "    add3((\"⊕\"))\n",
    "    add4((\"⊕\"))\n",
    "    add5((\"⊕\"))\n",
    "\n",
    "    %% Flows\n",
    "    txt1 --> id1 --> e1 --> add1 --> i1\n",
    "    txt2 --> id2 --> e2 --> add2 --> i2\n",
    "    txt3 --> id3 --> e3 --> add3 --> i3\n",
    "    txt4 --> id4 --> e4 --> add4 --> i4\n",
    "    txt5 --> id5 --> e5 --> add5 --> i5\n",
    "\n",
    "    p1 --> add1\n",
    "    p2 --> add2\n",
    "    p3 --> add3\n",
    "    p4 --> add4\n",
    "    p5 --> add5\n",
    "\n",
    "    input[\"...\"] --> inputtexts\n",
    "    inputembeddings --> transformer[\"Transformer\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1b6316-e35d-4d44-9f22-ff46349dd58f",
   "metadata": {},
   "source": [
    "### Context Vectors\n",
    "\n",
    "- AKA hidden states or contextual embeddings) are the outputs of the Transformer after self-attention has incorporated information from previous tokens.\n",
    "- RAW_input_embeddings = token_embeddings + positional_embeddings.\n",
    "- Transformer layer (decoder-only models) turn RAW_input_embeddings (input_embeddings) into context vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34676ea7-dfe5-4eff-9c71-4d3191675f9b",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "---\n",
    "title: \"1 to 1 mapping of: token -> ID -> embeddings -> context vectors\"\n",
    "---\n",
    "\n",
    "flowchart LR\n",
    "    subgraph inputtexts[\"Tokens\"]\n",
    "        direction TB\n",
    "        txt1[\"My\"]\n",
    "        txt2[\"name\"]\n",
    "        txt3[\"is\"]\n",
    "        txt4[\"Rob\"]\n",
    "        txt5[\".\"]\n",
    "    end\n",
    "\n",
    "    subgraph tokenids[\"Token IDs\"]\n",
    "        direction TB\n",
    "        id1[\"40134\"]\n",
    "        id2[\"2052\"]\n",
    "        id3[\"133\"]\n",
    "        id4[\"389\"]\n",
    "        id5[\"12\"]\n",
    "    end\n",
    "\n",
    "    subgraph tokenembeddings[\"Token Embeddings (Seq × Dim)\"]\n",
    "        direction TB\n",
    "        e1[\"t₁ = [0.12, -0.45, 0.67, ...]\"]\n",
    "        e2[\"t₂ = [0.34, 0.88, -0.12, ...]\"]\n",
    "        e3[\"t₃ = [0.05, 0.22, 0.91, ...]\"]\n",
    "        e4[\"t₄ = [0.77, -0.56, 0.11, ...]\"]\n",
    "        e5[\"t₅ = [0.03, 0.44, -0.78, ...]\"]\n",
    "    end\n",
    "\n",
    "    subgraph posembeddings[\"Positional Embeddings (Seq × Dim)\"]\n",
    "        direction TB\n",
    "        p1[\"p₁ = [0.01, 0.03, -0.02, ...]\"]\n",
    "        p2[\"p₂ = [0.05, -0.11, 0.08, ...]\"]\n",
    "        p3[\"p₃ = [-0.07, 0.04, 0.09, ...]\"]\n",
    "        p4[\"p₄ = [0.10, -0.02, 0.05, ...]\"]\n",
    "        p5[\"p₅ = [0.00, 0.12, -0.06, ...]\"]\n",
    "    end\n",
    "\n",
    "    subgraph inputembeddings[\"Input Embeddings (Token ⊕ Positional)\"]\n",
    "        direction TB\n",
    "        i1[\"t₁ + p₁\"]\n",
    "        i2[\"t₂ + p₂\"]\n",
    "        i3[\"t₃ + p₃\"]\n",
    "        i4[\"t₄ + p₄\"]\n",
    "        i5[\"t₅ + p₅\"]\n",
    "    end\n",
    "\n",
    "    subgraph contextvectors[\"Context Vectors (Seq × Dim after Transformer)\"]\n",
    "        direction TB\n",
    "        c1[\"c₁ = f(i₁)\"]\n",
    "        c2[\"c₂ = f(i₁,i₂)\"]\n",
    "        c3[\"c₃ = f(i₁,i₂,i₃)\"]\n",
    "        c4[\"c₄ = f(i₁,i₂,i₃,i₄)\"]\n",
    "        c5[\"c₅ = f(i₁,i₂,i₃,i₄,i₅)\"]\n",
    "    end\n",
    "\n",
    "    %% Addition nodes\n",
    "    add1((\"⊕\"))\n",
    "    add2((\"⊕\"))\n",
    "    add3((\"⊕\"))\n",
    "    add4((\"⊕\"))\n",
    "    add5((\"⊕\"))\n",
    "\n",
    "    %% Flows\n",
    "    txt1 --> id1 --> e1 --> add1 --> i1\n",
    "    txt2 --> id2 --> e2 --> add2 --> i2\n",
    "    txt3 --> id3 --> e3 --> add3 --> i3\n",
    "    txt4 --> id4 --> e4 --> add4 --> i4\n",
    "    txt5 --> id5 --> e5 --> add5 --> i5\n",
    "\n",
    "    p1 --> add1\n",
    "    p2 --> add2\n",
    "    p3 --> add3\n",
    "    p4 --> add4\n",
    "    p5 --> add5\n",
    "\n",
    "    input[\"...\"] --> inputtexts\n",
    "    inputembeddings --> transformer[\"Transformer\"]\n",
    "    transformer --> contextvectors\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2deda0cf-f3d4-47bd-8700-1b23490bcedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute context vectors\n",
    "\n",
    "inputs = torch.tensor([\n",
    "    [0.43, 0.15, 0.89],\n",
    "    [0.55, 0.87, 0.66],\n",
    "    [0.57, 0.85, 0.64],\n",
    "    [0.22, 0.58, 0.33],\n",
    "    [0.77, 0.25, 0.10],\n",
    "    [0.05, 0.80, 0.55]\n",
    "])\n",
    "\n",
    "attention_scores = torch.empty(6, 6)\n",
    "for i, x in enumerate(inputs):\n",
    "    for j, y in enumerate(inputs):\n",
    "        attention_scores[i, j] = torch.dot(x, y)\n",
    "\n",
    "# PyTorch overloads @ (idk how), i think new Python?\n",
    "# attention_scores = inptus @ inputs.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8fbd72a-787f-4410-a9cc-b30e035e10f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84342925-ec76-48ad-9cf0-9269d6ddeb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0000)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.0000)\n",
      "tensor(1.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "\n",
    "for row in attention_weights:\n",
    "    print(sum(row))\n",
    "\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e47234eb-1f22-423d-a2e9-fc9549992afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_context_vectors = attention_weights @ inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666df565-08cb-4eb3-b70c-f6958131ab99",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "\n",
    "---\n",
    "title: \"LLM forward pass: token → embeddings → attention → context → logits → probabilities\"\n",
    "---\n",
    "\n",
    "flowchart LR\n",
    "    subgraph inputtexts[\"Tokens\"]\n",
    "        direction TB\n",
    "        txt1[\"My\"]\n",
    "        txt2[\"name\"]\n",
    "        txt3[\"is\"]\n",
    "        txt4[\"Rob\"]\n",
    "        txt5[\".\"]\n",
    "    end\n",
    "\n",
    "    subgraph tokenids[\"Token IDs\"]\n",
    "        direction TB\n",
    "        id1[\"40134\"]\n",
    "        id2[\"2052\"]\n",
    "        id3[\"133\"]\n",
    "        id4[\"389\"]\n",
    "        id5[\"12\"]\n",
    "    end\n",
    "\n",
    "    subgraph tokenembeddings[\"Token Embeddings (Seq × Dim)\"]\n",
    "        direction TB\n",
    "        e1[\"t₁ = [0.12, -0.45, 0.67, ...]\"]\n",
    "        e2[\"t₂ = [0.34, 0.88, -0.12, ...]\"]\n",
    "        e3[\"t₃ = [0.05, 0.22, 0.91, ...]\"]\n",
    "        e4[\"t₄ = [0.77, -0.56, 0.11, ...]\"]\n",
    "        e5[\"t₅ = [0.03, 0.44, -0.78, ...]\"]\n",
    "    end\n",
    "\n",
    "    subgraph posembeddings[\"Positional Embeddings (Seq × Dim)\"]\n",
    "        direction TB\n",
    "        p1[\"p₁ = [0.01, 0.03, -0.02, ...]\"]\n",
    "        p2[\"p₂ = [0.05, -0.11, 0.08, ...]\"]\n",
    "        p3[\"p₃ = [-0.07, 0.04, 0.09, ...]\"]\n",
    "        p4[\"p₄ = [0.10, -0.02, 0.05, ...]\"]\n",
    "        p5[\"p₅ = [0.00, 0.12, -0.06, ...]\"]\n",
    "    end\n",
    "\n",
    "    subgraph inputembeddings[\"Input Embeddings (Token ⊕ Positional)\"]\n",
    "        direction TB\n",
    "        i1[\"t₁ + p₁\"]\n",
    "        i2[\"t₂ + p₂\"]\n",
    "        i3[\"t₃ + p₃\"]\n",
    "        i4[\"t₄ + p₄\"]\n",
    "        i5[\"t₅ + p₅\"]\n",
    "    end\n",
    "\n",
    "    subgraph attention[\"Self-Attention Mechanism\"]\n",
    "        direction TB\n",
    "        qk[\"Q · Kᵀ = Attention Scores\"]\n",
    "        soft[\"Softmax → Attention Weights (αᵢⱼ)\"]\n",
    "        weighted[\"Σ αᵢⱼ Vⱼ = Weighted Values\"]\n",
    "    end\n",
    "\n",
    "    subgraph contextvectors[\"Context Vectors (Seq × Dim after Transformer)\"]\n",
    "        direction TB\n",
    "        c1[\"c₁ = f(i₁)\"]\n",
    "        c2[\"c₂ = f(i₁,i₂)\"]\n",
    "        c3[\"c₃ = f(i₁,i₂,i₃)\"]\n",
    "        c4[\"c₄ = f(i₁,i₂,i₃,i₄)\"]\n",
    "        c5[\"c₅ = f(i₁,i₂,i₃,i₄,i₅)\"]\n",
    "    end\n",
    "\n",
    "    subgraph outputlogits[\"Output Layer\"]\n",
    "        direction TB\n",
    "        linear[\"Linear Projection (Wᵒᵘᵗ) → Logits\"]\n",
    "        vocabsoft[\"Softmax over Vocab → Next-token Probabilities\"]\n",
    "    end\n",
    "\n",
    "    %% Addition nodes\n",
    "    add1((\"⊕\"))\n",
    "    add2((\"⊕\"))\n",
    "    add3((\"⊕\"))\n",
    "    add4((\"⊕\"))\n",
    "    add5((\"⊕\"))\n",
    "\n",
    "    %% Flows\n",
    "    txt1 --> id1 --> e1 --> add1 --> i1\n",
    "    txt2 --> id2 --> e2 --> add2 --> i2\n",
    "    txt3 --> id3 --> e3 --> add3 --> i3\n",
    "    txt4 --> id4 --> e4 --> add4 --> i4\n",
    "    txt5 --> id5 --> e5 --> add5 --> i5\n",
    "\n",
    "    p1 --> add1\n",
    "    p2 --> add2\n",
    "    p3 --> add3\n",
    "    p4 --> add4\n",
    "    p5 --> add5\n",
    "\n",
    "    input[\"...\"] --> inputtexts\n",
    "    inputembeddings --> attention\n",
    "    attention --> contextvectors\n",
    "    contextvectors --> linear --> vocabsoft\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd94dc75-d9b7-4971-ae36-abbcffef5289",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
